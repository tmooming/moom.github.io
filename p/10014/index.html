<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.jpg"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg"><link rel="mask-icon" href="/images/favicon.jpg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.turuwei.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta property="og:type" content="article"><meta property="og:title" content="《神经网络与机器学习》笔记（一）"><meta property="og:url" content="http://www.turuwei.com/p/10014/index.html"><meta property="og:site_name" content="TuRuwei&#39;s Blog"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://upload-images.jianshu.io/upload_images/16788405-92f59abf85a514b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="https://upload-images.jianshu.io/upload_images/16788405-cf9da67b4d70e5b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="https://upload-images.jianshu.io/upload_images/16788405-5ebc8d85449e6e4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="https://upload-images.jianshu.io/upload_images/16788405-e5d2f2987df593e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="article:published_time" content="2021-02-25T03:44:54.000Z"><meta property="article:modified_time" content="2021-02-25T08:13:19.798Z"><meta property="article:author" content="tmooming"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/16788405-92f59abf85a514b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><link rel="canonical" href="http://www.turuwei.com/p/10014/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>《神经网络与机器学习》笔记（一） | TuRuwei's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="TuRuwei's Blog" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">TuRuwei's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">爱学习，爱生活</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-fa fa-home"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-fa fa-user"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-fa fa-tags"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-fa fa-th"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-fa fa-archive"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/tmooming" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://www.turuwei.com/p/10014/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="tmooming"><meta itemprop="description" content="吾生也有涯，而知也无涯"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="TuRuwei's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">《神经网络与机器学习》笔记（一）</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-25 11:44:54 / 修改时间：16:13:19" itemprop="dateCreated datePublished" datetime="2021-02-25T11:44:54+08:00">2021-02-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">读书笔记</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/p/10014/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/p/10014/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.7k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><img src="https://upload-images.jianshu.io/upload_images/16788405-92f59abf85a514b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="本书组织架构"></p><span id="more"></span><h2 id="入门篇"><a href="#入门篇" class="headerlink" title="入门篇"></a>入门篇</h2><hr><h3 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章  绪论"></a>第一章 绪论</h3><h4 id="特征表示方法"><a href="#特征表示方法" class="headerlink" title="特征表示方法"></a>特征表示方法</h4><p><strong>局部特征</strong></p><blockquote><p>含义：也称为<em>离散表示</em>或<em>符号表示</em>，通常是用one-hot向量的形式</p><p>优点：</p><ol><li>这种离散的表示方式具有很好的解释性</li><li>因为向量稀疏，所以用于线性模型时计算效率非常高</li></ol><p>缺点：</p><ol><li>one-hot向量维数太高，不能扩展（因为维数是由词表大小决定的）</li><li>不同向量之间的相似度为0，无法进行相似度计算</li></ol></blockquote><p><strong>分布式表示</strong></p><blockquote><p>含义：也称为*分散式表示，如NLP中的词嵌入，是用语义空间的基向量进行表示的</p><p>优点：</p><ol><li>可以表示成低维的稠密向量</li><li>表示能力强，维数可以指定</li><li>相似度容易计算</li></ol><p>缺点：</p><ol><li>解释性不强</li></ol></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/16788405-cf9da67b4d70e5b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="深度学习需要解释的问题是：贡献度分配问题
"></p><p>​</p><h3 id="第二章-机器学习概述"><a href="#第二章-机器学习概述" class="headerlink" title="第二章  机器学习概述"></a>第二章 机器学习概述</h3><p><strong>损失函数</strong></p><blockquote><script type="math/tex;mode=display">\begin{aligned} \mathcal{L}(y, f(x ; \theta)) &=\left\{\begin{array}{ll}{0} & {\text { if } y=f(x ; \theta)} \\ {1} & {\text { if } y \neq f(x ; \theta)}\end{array}\right.\\ &=I(y \neq f(x ; \theta)) \end{aligned}\\
0-1损失函数，能够客观的评价模型好坏，但数学性质不好，不连续且导数部位0，难以优化。</script><script type="math/tex;mode=display">平方损失函数：\mathcal{L}(y, f(x ; \theta))=\frac{1}{2}(y-f(x ; \theta))^{2}\\
经常用在预测标签y 为实数值的任务中，平方损失函数一般不适用于分类问题。因为输入和输出不为连续值。</script><script type="math/tex;mode=display">交叉熵损失函数：一般用于分类问题（衡量两个概率分布的差异：即标签真实分布y和模型预测分布f(x;\theta)之间的交叉熵）\\
\mathcal{L}(\boldsymbol{y}, f(\boldsymbol{x} ; \theta))=-\sum_{c=1}^{C} y_{c} \log f_{c}(\boldsymbol{x} ; \theta)\\
由上公式可知，交叉熵损失函数也就是 负对数似然损失函数</script><script type="math/tex;mode=display">Hinge损失函数：在二分类问题中，假设y的取值为{-1，+1}，f(x;\theta)\in\mathbb{R}:\\
\begin{aligned} \mathcal{L}(y, f(x ; \theta)) &=\max (0,1-y f(x ; \theta)) \\ & \triangleq[1-y f(x ; \theta)]_{+} \end{aligned}    \\
其中[x]_+=max(0,x)</script></blockquote><p><strong>优化方法</strong></p><blockquote><p>批量梯度下降算法：计算量太大</p><p>随机梯度下降算法：无法利用计算机的并行性</p><p>小批量梯度下降法：前两者的折中，是目前最主要的优化算法</p></blockquote><p><strong>参数学习</strong></p><blockquote><p>机器学习任务可以分为两类： 一类是样本的特征向量x 和标签y 之间存在未知的函数关系y = h(x)，另一类是条件概率p(y|x) 服从某个未知分布。最小二乘法是属于第一类，直接建模x 和标签y 之间的函数关系。此外，线性回归还可以通过建模条件概率p(y|x) 的角度来进行参数估计。</p></blockquote><p>经验风险最小化</p><blockquote><p>可以用平方损失函数来计算：</p><script type="math/tex;mode=display">\begin{aligned} \mathcal{R}(\boldsymbol{w}) &=\sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(\boldsymbol{x}^{(n)} ; \boldsymbol{w}\right)\right) \\ &=\frac{1}{2} \sum_{n=1}^{N}\left(y^{(n)}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}^{(n)}\right)^{2} \\ &=\frac{1}{2}\left\|\boldsymbol{y}-X^{\mathrm{T}} \boldsymbol{w}\right\|^{2} \end{aligned}\\
用最小二乘法求得最优参数：\begin{aligned} w^{*} &=\left(X X^{\mathrm{T}}\right)^{-1} X y &=\left(\sum_{n=1}^{N}x^{(n)}\left(x^{(n)}\right)^{\mathrm{T}}\right)^{-1}\left(\sum_{n=1}^{N} x^{(n)} y^{(n)}\right) \end{aligned}\\
用最小二乘法进行计算有个限制:XX^T必须存在逆矩阵，也就是说X中得行向量必须是线性无关的。\\
当XX^T不可逆时，有两种方法：1)使用主成分分析等方法先预处理数据，消除特征间的相关性，再用最小二乘法。\\
2)通过梯度下降法来估计参数，然后用最小均方(LMS)法:w \leftarrow w+\alpha X\left(y-X^{\mathrm{T}} w\right)</script></blockquote><p>结构风险最小化</p><blockquote><p>为了解决最小二乘法中，因为特征间的多重共线性而导致计算不准确的问题，出现了岭回归，即给XX^T^的对角线元素都加上一个常数λ使得（XX^T^+λI）满秩。从而最优参数<script type="math/tex">w^*=(XX^T+\lambda I)^{-1}X y</script>.</p><p>岭回归可以看作是结构风险最小化准则下的最小二乘法估计。且其目标函数可以写为：</p><script type="math/tex;mode=display">\mathcal{R}(\boldsymbol{w})=\frac{1}{2}\left\|\boldsymbol{y}-X^{\mathrm{T}} \boldsymbol{w}\right\|^{2}+\frac{1}{2} \lambda\|\boldsymbol{w}\|^{2}</script></blockquote><h3 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h3><p>四种不同的线性分类模型：Logistic回归、Softmax回归、感知机和支持向量机</p><h4 id="线性判别函数和决策边界"><a href="#线性判别函数和决策边界" class="headerlink" title="线性判别函数和决策边界"></a>线性判别函数和决策边界</h4><blockquote><p>最简单的是二分类：只需要一个线性判别函数：<script type="math/tex">f(x;w)=w^Tx+b</script>。决策边界就是特征空间<script type="math/tex">\mathbb{R}^{d}</script>中所有满足<script type="math/tex">f(x;w)=0</script>的点组成的一个分割超平面。</p><p>多分类：是指分类的类别数C大于2。设计多分类的判别函数有三种常用方法：</p><ol><li><p>“一对其余”方式，也就是转化成C个“一对其余”的二分类问题，这需要C个判别函数。</p></li><li><p>“一对一”方式，就是转化成C(C-1)/2个“一对一的”二分类问题。。这种方式共需要C(C − 1)/2 个判别函数，其中第(i, j) 个判别函数是把类别 i 和类别 j 的样本分开。</p></li><li><p>“argmax”方式：是一种改进的“一对其余”方式，需要C个判别函数：</p><script type="math/tex;mode=display">f_c(x;w_c)=w^T_cx+b_c,    c=[1,\dots,C]$$，对于样本x，如果存在一个类别c，相对于所有的其他类别˜c(˜c≠ c) 有f~c~(x;w~c~) > f~˜c~(x,w~˜c~)，那么x 属于类别c。“argmax”方式的预测函数定义为:

$$y=argmax^C_{c=1}f_c(x;w_c)</script></li></ol><p>“一对其余”方式和“一对一”方式都存在一个缺陷：特征空间中会存在一些 难以确定类别的区域，而“argmax”方式很好地解决了这个问题：</p><p><img src="https://upload-images.jianshu.io/upload_images/16788405-5ebc8d85449e6e4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p></blockquote><h4 id="Logistic-回归"><a href="#Logistic-回归" class="headerlink" title="Logistic 回归"></a>Logistic 回归</h4><p>是一种常用的处理二分类问题的线性模型。</p><p>为了解决连续的线性函数不适合进行分类的问题，我们引入非线性函数<script type="math/tex">g:\mathbb{R}^d\rightarrow(0,1)</script>来预测类别标签的后验概率p(y = 1|x)。<script type="math/tex">p(y=1|x)=g(f(x;w))</script>。</p><p>其中<script type="math/tex">g(\cdot)</script>通常被称为激活函数，其作用是把线性函数的值域从实数“挤压”到(0,1)之间，可以用来表示概率。。在统计文献中，g(·) 的逆函数g^−1^(·) 也称为联系函数（Link Function）。</p><p>在Logistic 回归中，激活函数就是Logistic 函数，标签y=1的后验概率为：<script type="math/tex">p(y=1|x)=\sigma\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}\right)\triangleq \frac{1}{1+\exp \left(-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}\right)}</script>。</p><p>Logistic 回归采用交叉熵作为损失函数：</p><script type="math/tex;mode=display">\mathcal{R}(w)=-\frac{1}{N} \sum_{n=1}^{N}\left(p_{r}\left(y^{(n)}=1 | x^{(n)}\right) \log \hat{y}^{(n)}+p_{r}\left(y^{(n)}=0 | x^{(n)}\right) \log \left(1-\hat{y}^{(n)}\right)\right)\\=-\frac{1}{N} \sum_{n=1}^{N}\left(y^{(n)} \log \hat{y}^{(n)}+\left(1-y^{(n)}\right) \log \left(1-\hat{y}^{(n)}\right)\right)</script><p>采用梯度下降法来对参数进行优化：<script type="math/tex">\boldsymbol{w}_{t+1} \leftarrow \boldsymbol{w}_{t}+\alpha \frac{1}{N} \sum_{n=1}^{N} \boldsymbol{x}^{(n)}\left(y^{(n)}-\hat{y}_{\boldsymbol{w}_{t}}^{(n)}\right)</script></p><h4 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h4><p>也称为多项或多类的Logistic 回归，是Logistic 回归在多分类问题上的推广。</p><p>对于多类问题，类别标签y ∈ {1, 2, · · · ,C}可以有C个取值。y是用one-hot向量表示的，给定一个样本x，Softmax 回归预测的属于类别c 的条件概率为：</p><script type="math/tex;mode=display">\begin{aligned} p(y=c | \boldsymbol{x}) &=\operatorname{softmax}\left(\boldsymbol{w}_{c}^{\mathrm{T}} \boldsymbol{x}\right) \\ &=\frac{\exp \left(\boldsymbol{w}_{c}^{\mathrm{T}} \boldsymbol{x}\right)}{\sum_{c^{\prime}=1}^{C} \exp \left(\boldsymbol{w}_{c^{\prime}}^{\mathrm{T}} \boldsymbol{x}\right)} \end{aligned}</script><p>其中w~c~是第c类的权重向量。上述公式向量表示为：</p><script type="math/tex;mode=display">\begin{aligned} \hat{\boldsymbol{y}} &=\operatorname{softmax}\left(W^{\mathrm{T}} \boldsymbol{x}\right) =\frac{\exp \left(W^{\mathrm{T}} \boldsymbol{x}\right)}{1^{\mathrm{T}} \exp \left(W^{\mathrm{T}} \boldsymbol{x}\right)} \end{aligned}   ，1为全1向量</script><p>Softmax 回归的决策函数可以表示为：</p><script type="math/tex;mode=display">\begin{aligned} \hat{y} &=\underset{c=1}{\arg \max } p(y=c | \boldsymbol{x}) \\ &=\underset{c=1}{\arg \max } \boldsymbol{w}_{c}^{\mathrm{T}} \boldsymbol{x} \end{aligned}</script><p>与Logistic 回归的关系：当类别数C = 2 时，Softmax 回归的决策函数为：</p><script type="math/tex;mode=display">\begin{aligned} \hat{y} &=\underset{y \in\{0,1\}}{\arg \max } \boldsymbol{w}_{y}^{\mathrm{T}} \boldsymbol{x} \\ &=I\left(\boldsymbol{w}_{1}^{\mathrm{T}} \boldsymbol{x}-\boldsymbol{w}_{0}^{\mathrm{T}} \boldsymbol{x}>0\right) =I\left(\left(\boldsymbol{w}_{1}-\boldsymbol{w}_{0}\right)^{\mathrm{T}} \boldsymbol{x}>0\right) \end{aligned}</script><p>二分类中的权重向量<script type="math/tex">w=w_1-w_0</script></p><p>Softmax 回归也采用交叉熵损失函数，其风险函数为：</p><script type="math/tex;mode=display">\begin{aligned} \mathcal{R}(W) &=-\frac{1}{N} \sum_{n=1}^{N} \sum_{c=1}^{C} \boldsymbol{y}_{c}^{(n)} \log \hat{\boldsymbol{y}}_{c}^{(n)} =-\frac{1}{N} \sum_{n=1}^{N}\left(\boldsymbol{y}^{(n)}\right)^{\mathrm{T}} \log \hat{\boldsymbol{y}}^{(n)} \end{aligned}\\其中\hat{y}^{(n)} = softmax(W^Tx^{(n)}) 为样本x^{(n)} 在每个类别的后验概率</script><p>风险函数<script type="math/tex">\mathcal{R}(W)  关于W 的梯度为
\frac{\partial \mathcal{R}(W)}{\partial W}=-\frac{1}{N} \sum_{n=1}^{N} x^{(n)}\left(\boldsymbol{y}^{(n)}-\hat{\boldsymbol{y}}^{(n)}\right)^{\mathrm{T}}</script></p><script type="math/tex;mode=display">采用梯度下降法，Softmax 回归的训练过程为：初始化W~0~ ← 0，然后通过下式进行迭代更新：\\
W_{t+1} \leftarrow W_{t}+\alpha\left(\frac{1}{N} \sum_{n=1}^{N} x^{(n)}\left(y^{(n)}-\hat{y}_{W_{t}}^{(n)}\right)^{\mathrm{T}}\right)</script><h4 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h4><p>是一种线性分类器，也是最简单的人工神经网络</p><p>感知器的学习算法是一种错误驱动的在线学习算法。先初始化一个权重向量<script type="math/tex">w\leftarrow0</script>（通常是全零向量），然后每次分错一个样本(x, y)时，即<script type="math/tex">yw^Tx<0</script>，就用这个样本来更新权重,采用随机梯度下降。<script type="math/tex">w\leftarrow w+yx</script></p><p>但是感知器的权重更新与样本的顺序有关，只要每次迭代的顺序不一致时，找到的分割超平面也往往不一致。同时，如果训练集不是线性可分的，就永远不会收敛。</p><p>为解决对样本顺序的依赖问题，提出了参数平均感知器。</p><p>同时，将感知器在多分类上进行扩展，可以得到广义感知器。</p><h4 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h4><p>参照《西瓜书》</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p><img src="https://upload-images.jianshu.io/upload_images/16788405-e5d2f2987df593e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>tmooming</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://www.turuwei.com/p/10014/" title="《神经网络与机器学习》笔记（一）">http://www.turuwei.com/p/10014/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-item"><a href="/p/10012/" rel="prev" title="如何在无限序列中随机抽取元素"><i class="fa fa-chevron-left"></i> 如何在无限序列中随机抽取元素</a></div><div class="post-nav-item"></div></div></footer></article><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束 <i class="fa fa-heart"></i> 感谢阅读-------------</div></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:t}=CONFIG.comments;if(CONFIG.comments.storage&&(t=localStorage.getItem("comments_active")||t),t){let e=document.querySelector(`a[href="#comment-${t}"]`);e&&e.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{if(!t.target.matches(".tabs-comment .tab-content .tab-pane"))return;let e=t.target.classList[1];localStorage.setItem("comments_active",e)})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A5%E9%97%A8%E7%AF%87"><span class="nav-text">入门篇</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E7%BB%AA%E8%AE%BA"><span class="nav-text">第一章 绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="nav-text">特征表示方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-text">第二章 机器学习概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">第三章 线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%E5%92%8C%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="nav-text">线性判别函数和决策边界</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-%E5%9B%9E%E5%BD%92"><span class="nav-text">Logistic 回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax-%E5%9B%9E%E5%BD%92"><span class="nav-text">Softmax 回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-text">感知器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-text">支持向量机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="tmooming" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">tmooming</p><div class="site-description" itemprop="description">吾生也有涯，而知也无涯</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">11</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">27</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/tmooming" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;tmooming" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:tmooming@163.com" title="E-Mail → mailto:tmooming@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script><script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script><div class="widget-wrap"><h3 class="widget-title">Tag Cloud</h3><div id="myCanvasContainer" class="widget tagcloud"><canvas width="250" height="250" id="resCanvas"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Net/" rel="tag">.Net</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataframe/" rel="tag">Dataframe</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jupyter/" rel="tag">Jupyter</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kitematic/" rel="tag">Kitematic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pycharm/" rel="tag">Pycharm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/" rel="tag">Ubuntu</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/anaconda3/" rel="tag">anaconda3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/apache/" rel="tag">apache</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/codewars/" rel="tag">codewars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gitalk/" rel="tag">gitalk</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/next/" rel="tag">next</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/" rel="tag">pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pgadmin4/" rel="tag">pgadmin4</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/postgresql/" rel="tag">postgresql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows/" rel="tag">windows</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E7%A1%80/" rel="tag">基础</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B9%B6%E8%A1%8C/" rel="tag">并行</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="tag">环境配置</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%AD%E6%B3%95/" rel="tag">语法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a><span class="tag-list-count">1</span></li></ul></canvas></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">tmooming</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span title="站点总字数">24k</span><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("01/01/2020 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已安全运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">22 分钟</span></div><div class="BbeiAn-info" style="color:#000">粤ICP备 - <a target="_blank" href="http://www.miitbeian.gov.cn/" style="color:#000" rel="nofollow">2020140626</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const a=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],a),d=document.createTextNode("");t.parentNode.replaceChild(d,t),n.start={node:d,delim:"",n:0},n.end={node:d,delim:"",n:0},e.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",()=>{var e=["nick","mail","link"],i="nick,mail,link";i=i.split(",").filter(i=>e.includes(i)),new Valine({el:"#valine-comments",verify:!1,notify:!1,appId:"G8l8fYYswIllfITfbLbKxxiS-gzGzoHsz",appKey:"ccAz1rmxE4kzTn8gVs7b4SJh",placeholder:"欢迎畅所欲言",avatar:"mm",meta:i,pageSize:"10",visitor:!1,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>